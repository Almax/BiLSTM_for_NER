{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sag/src_George/workspace/virtualenv/lib/python3.4/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import gzip\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim\n",
    "from itertools import islice\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from conllu.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.Word2Vec.load_word2vec_format('./wikiru.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['лафета\\xa0—',\n",
       " 'мартыном',\n",
       " 'суламиты',\n",
       " 'bit»',\n",
       " 'семиосоциопсихологии',\n",
       " 'полнометражном',\n",
       " 'будерим',\n",
       " 'попахивает',\n",
       " 'краям»',\n",
       " 'тилия',\n",
       " 'саo',\n",
       " '\\xa0якушкиным',\n",
       " 'цервикальная',\n",
       " 'кловеp',\n",
       " 'оказавшемуся',\n",
       " 'приплывёт',\n",
       " 'dnow',\n",
       " 'рабельо',\n",
       " 'сибутрамина',\n",
       " 'прерывая',\n",
       " 'kg/default',\n",
       " 'ксизовского',\n",
       " 'вьючная',\n",
       " 'проследовала',\n",
       " 'псикосмологию',\n",
       " 'овендена',\n",
       " 'королёве»',\n",
       " 'ариевич',\n",
       " 'премьерному',\n",
       " 'всеволодово',\n",
       " 'аддукция',\n",
       " 'fingertips',\n",
       " 'колитах',\n",
       " 'нейронов\\xa0—',\n",
       " 'понтификов',\n",
       " 'вихревой',\n",
       " 'камийена',\n",
       " 'amsterdam»\\xa0—',\n",
       " 'schneider»',\n",
       " 'боннелл',\n",
       " 'лыжин',\n",
       " 'окислов',\n",
       " 'пикве',\n",
       " 'organy',\n",
       " 'понятийной',\n",
       " 'ужаснейших',\n",
       " 'испаряться',\n",
       " 'разбойничавших',\n",
       " 'аналити́ческая',\n",
       " 'стеффани',\n",
       " 'nationalencyklopedin',\n",
       " 'торжествовали',\n",
       " 'кащее',\n",
       " 'чего…',\n",
       " 'нельзя…',\n",
       " 'насаждений\\xa0—',\n",
       " 'coast»',\n",
       " 'топирамат',\n",
       " 'тримарана',\n",
       " 'юрьянском',\n",
       " 'тюлевый',\n",
       " 'отдела\\xa0—',\n",
       " 'апеннинских',\n",
       " 'строить\\xa0—',\n",
       " 'возобновятся',\n",
       " 'скорости,',\n",
       " 'металлика»',\n",
       " 'да́угава',\n",
       " 'вахмана',\n",
       " 'рубэцу',\n",
       " 'кабашев',\n",
       " 'хоуген',\n",
       " 'лемша',\n",
       " 'вервольфов',\n",
       " 'слогану',\n",
       " 'бантом\\xa0—',\n",
       " 'военинженером',\n",
       " 'виксбергской',\n",
       " 'стяга',\n",
       " 'richy',\n",
       " 'придонным',\n",
       " 'гунона',\n",
       " 'выраженных',\n",
       " 'познающий',\n",
       " 'niketas',\n",
       " 'поморник',\n",
       " 'амурозавра',\n",
       " 'scheibler',\n",
       " 'азовом',\n",
       " 'швракич',\n",
       " 'мунгёнсэджэ',\n",
       " 'sathya',\n",
       " 'дюрас',\n",
       " 'ардан\\xa0—',\n",
       " 'проверятся',\n",
       " 'титуловавшие',\n",
       " 'гроция»',\n",
       " 'резниченко»',\n",
       " 'opening\\xa0—',\n",
       " 'интегрировавшихся',\n",
       " 'кокиль',\n",
       " 'спонвиль',\n",
       " 'билич',\n",
       " 'ведающих',\n",
       " 'царевококшайского',\n",
       " 'кампусу',\n",
       " '\\\\frac,\\\\',\n",
       " 'мнимыми',\n",
       " 'сухаревы',\n",
       " 'melanopterus',\n",
       " 'янаева',\n",
       " 'соон',\n",
       " 'вязкими',\n",
       " 'софіївська',\n",
       " 'качели\\xa0—',\n",
       " 'агропромэнерго»',\n",
       " 'зию',\n",
       " 'ashdown',\n",
       " 'фиксируемые',\n",
       " 'птеродактиль',\n",
       " 'мастюгинское',\n",
       " 'жульничал',\n",
       " 'творить»',\n",
       " 'alius',\n",
       " 'всемирноизвестной',\n",
       " 'белоярском',\n",
       " 'подзадача',\n",
       " 'freeвольная',\n",
       " 'будённовцев',\n",
       " 'вставилась',\n",
       " 'полубезумного',\n",
       " 'парома»',\n",
       " 'казевел',\n",
       " 'завидев',\n",
       " 'голенювский',\n",
       " 'бульверова',\n",
       " '#оспа',\n",
       " 'памирцы',\n",
       " 'питерскую',\n",
       " 'треворе',\n",
       " 'жексенбаева',\n",
       " 'автолитических',\n",
       " 'балсас',\n",
       " 'mesr',\n",
       " 'schl',\n",
       " 'грациозностью',\n",
       " 'хессин',\n",
       " 'sickest',\n",
       " 'перенасыщения',\n",
       " 'gaviria',\n",
       " 'семижаберные',\n",
       " 'шарета',\n",
       " 'кордяжское',\n",
       " 'земель»\\xa0—',\n",
       " 'мусульманам',\n",
       " 'rakete',\n",
       " 'бобами',\n",
       " 'зольники',\n",
       " 'междуречья',\n",
       " 'лучино',\n",
       " 'развалами',\n",
       " 'солидуса',\n",
       " 'рапидографы',\n",
       " 'сётакона',\n",
       " 'круглосуточные',\n",
       " 'пронесся',\n",
       " 'опылить',\n",
       " 'очерчена',\n",
       " 'агитирующих',\n",
       " 'белоглазов',\n",
       " 'кейро',\n",
       " 'глазчатых',\n",
       " 'ацтекских',\n",
       " 'сдаются',\n",
       " 'техническая\\xa0—',\n",
       " 'косинец',\n",
       " 'magazine/re',\n",
       " 'свабирование',\n",
       " 'траншеях',\n",
       " 'теодорович',\n",
       " 'бартельми',\n",
       " 'спецгашение',\n",
       " 'mauritiana',\n",
       " 'burchell',\n",
       " 'албунея',\n",
       " 'zabranjeno',\n",
       " 'темноват',\n",
       " 'костылёвка',\n",
       " 'развевающегося',\n",
       " 'подпустив',\n",
       " 'праксилла',\n",
       " 'полибинское',\n",
       " 'губчатый',\n",
       " 'ушибе',\n",
       " 'баланс»',\n",
       " 'дощатый',\n",
       " 'кастреном',\n",
       " 'выплёвывать',\n",
       " 'authoritarian',\n",
       " 'кибальчичу',\n",
       " 'vietnama',\n",
       " 'самородную',\n",
       " 'мурриете',\n",
       " 'лувсанжамцын',\n",
       " 'антитеррор»\\xa0—',\n",
       " 'vadalis',\n",
       " 'римским',\n",
       " 'verliebt',\n",
       " 'монашеские',\n",
       " 'манди']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "list(islice(w2v_model.vocab, 20000, 20210))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pos_tags = {'A': 'A',\n",
    " 'ADV': 'ADV',\n",
    " 'COM': 'COM',\n",
    " 'CONJ': 'CONJ',\n",
    " 'ENG': 'UNKN',\n",
    " 'INTJ': 'INTJ',\n",
    " 'MAD': 'UNKN',\n",
    " 'MID': 'UNKN',\n",
    " 'NUM': 'NUM',\n",
    " 'PART': 'PART',\n",
    " 'PR': 'PR',\n",
    " 'S': 'S',\n",
    " 'UNKNW': 'UNKN',\n",
    " 'V': 'V',\n",
    " 'ВИН': 'UNKN',\n",
    " 'ЖЕН': 'UNKN',\n",
    " 'ИМ': 'UNKN',\n",
    " 'МУЖ': 'UNKN',\n",
    " 'ПР': 'UNKN',\n",
    " 'РОД': 'UNKN',\n",
    " 'СРЕД': 'UNKN',\n",
    " 'ТВОР': 'UNKN',\n",
    " 'мд2': 'UNKN',\n",
    " 'мн17': 'UNKN',\n",
    " 'х5': 'UNKN'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "label2Idx = {'LOC': 0, 'MISC': 3, 'O': 3, 'ORG': 1, 'PER': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOC': 0, 'MISC': 3, 'O': 3, 'ORG': 1, 'PER': 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2Idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_sentences(feature_data):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    i = 0\n",
    "    feature_data = feature_data.fillna(0)\n",
    "    for row in feature_data.iterrows():\n",
    "        if row[1]['lemma'] == 0:\n",
    "            continue\n",
    "        if row[1]['isFirstWord'] == 1 and i != 0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "        i = 1\n",
    "        sentence.append(row[1])\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_feature_data_ud(data):\n",
    "    l = []\n",
    "    labelSet= set()\n",
    "    for sent in data:\n",
    "        for ind, word in enumerate(sent):\n",
    "            w = []\n",
    "            labelSet.add(word['upostag'])\n",
    "            w.append(word['upostag'])\n",
    "            w.append(word['form'])\n",
    "            w.append(word['lemma'])\n",
    "            w.append(int(word['form'].isupper()))\n",
    "            w.append(int(word['form'].istitle()))\n",
    "            w.append(int(word['form'].islower()))\n",
    "            w.append(int(word['form'].isdigit()))\n",
    "            w.append(int(word['form'].isalpha()))\n",
    "            if ind == len(sent)-2 or ind == len(sent)-1:\n",
    "                w.append(1)\n",
    "            else:\n",
    "                w.append(0)\n",
    "            if ind == 0:\n",
    "                w.append(1)\n",
    "            else:\n",
    "                w.append(0)\n",
    "            l.append(w)\n",
    "    print('ok2')\n",
    "    feature_data = pd.DataFrame(l, columns=['pos', 'forma', 'lemma', 'isupper', 'istitle', 'islower', 'isdigit', 'isalpha', 'isLastWord', 'isFirstWord'])\n",
    "    return feature_data, labelSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_data():\n",
    "    folder = '../data/'\n",
    "    feature_file = '_final.features.csv'\n",
    "    target_file = '.targets.csv'\n",
    "    files = ['Wiki_full/WikiNER_part1', 'Wiki_full/WikiNER_part2', 'Wiki_full/WikiNER_part3', \\\n",
    "        'Wiki_full/WikiNER_part4', 'Wiki_full/WikiNER_part5', 'Wiki_full/WikiNER_part6', 'Wiki_full/WikiNER_part7', \\\n",
    "        'Wiki_full/WikiNER_part8', 'Wiki_full/WikiNER_part9', 'Wiki_full/WikiNER_part10', 'Wiki_full/WikiNER_part11', \\\n",
    "        'Wiki_full/WikiNER_part12', 'Wiki_full/WikiNER_part13', 'Wiki_full/WikiNER_part14', 'Wiki_full/WikiNER_part15',    'Wiki_full/WikiNER_part16', 'Wiki_full/WikiNER_part17', 'Wiki_full/WikiNER_part18', 'Wiki_full/WikiNER_part19',    'Wiki_full/WikiNER_part20']\n",
    "    feature_data = pd.DataFrame()\n",
    "    for f in files:\n",
    "        df_train_all = pd.read_csv(folder + f + feature_file, sep=';', usecols=['forma', 'pos','link', 'len', 'lemma', 'isupper', 'istitle', 'islower', 'isdigit', 'isalpha', 'isalnum', 'isLastWord', 'isFirstWord', 'grm', 'posStartInText'])\n",
    "        df_train_target = pd.read_csv(folder + f + target_file, sep=';')\n",
    "        #df_train_t = df_train_all[['pos','link', 'len', 'lemma', 'isupper', 'istitle', 'islower', 'isdigit', 'isalpha', 'isalnum', 'isLastWord', 'isFirstWord', 'grm', 'posStartInText']]\n",
    "        df_train_t = pd.concat([df_train_all, df_train_target], axis=1)\n",
    "        feature_data = pd.concat([feature_data, df_train_t], axis=0)\n",
    "        del df_train_all, df_train_target, df_train_t\n",
    "        print('ok')\n",
    "    print('ok2')\n",
    "    \n",
    "    return feature_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dial_sentences():\n",
    "    folder = '../data/'\n",
    "    feature_file = '.features.csv'\n",
    "    target_file = '.targets.csv'\n",
    "    files = ['Dialog_test/NER_testset', 'Dialog_train/NER_devset']\n",
    "    df_train_all = pd.read_csv(folder + files[1] + feature_file, sep=';')\n",
    "    df_test_all = pd.read_csv(folder + files[0] + feature_file, sep=';')\n",
    "    df_train_target = pd.read_csv(folder + files[1] + target_file, sep=';')\n",
    "    df_test_target = pd.read_csv(folder + files[0] + target_file, sep=';')\n",
    "    df_train_t = df_train_all[['forma', 'pos','link', 'len', 'lemma', 'isupper', 'istitle', 'islower', 'isdigit', 'isalpha', 'isalnum', 'isLastWord', 'isFirstWord', 'grm', 'posStartInText', 'posStart']]\n",
    "    df_test_t = df_test_all[['forma', 'pos','link', 'len', 'lemma', 'isupper', 'istitle', 'islower', 'isdigit', 'isalpha', 'isalnum', 'isLastWord', 'isFirstWord', 'grm', 'posStartInText', 'posStart']]\n",
    "    df_train = pd.concat([df_train_t, df_train_target], axis=1)\n",
    "    df_test = pd.concat([df_test_t, df_test_target], axis=1)\n",
    "    train_sentences_dial = get_sentences(df_train)\n",
    "    test_sentences_dial = get_sentences(df_test)\n",
    "    return train_sentences_dial, test_sentences_dial\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wiki_sentences():\n",
    "    \n",
    "    \n",
    "    #files = ['Wiki_full/WikiNER_part1', 'Wiki_full/WikiNER_part2', 'Wiki_full/WikiNER_part3', \\\n",
    "    #    'Wiki_full/WikiNER_part4', 'Wiki_full/WikiNER_part5', 'Wiki_full/WikiNER_part6', 'Wiki_full/WikiNER_part7', \\\n",
    "    #    'Wiki_full/WikiNER_part8', 'Wiki_full/WikiNER_part9', 'Wiki_full/WikiNER_part10']\n",
    "\n",
    "    feature_data = get_feature_data()\n",
    "    X_train, X_test = train_test_split(feature_data, test_size=0.25, random_state=42)\n",
    "    train_sentences = get_sentences(X_train)\n",
    "    test_sentences = get_sentences(X_test)\n",
    "    return train_sentences, test_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ud_sentences():\n",
    "    folder = '../data/'\n",
    "    files_ud = ['UD_Russian-SynTagRus/ru_syntagrus-ud-dev.conllu', 'UD_Russian-SynTagRus/ru_syntagrus-ud-test.conllu', \\\n",
    "           'UD_Russian-SynTagRus/ru_syntagrus-ud-train.conllu']\n",
    "    data = []\n",
    "    for fl in files_ud:\n",
    "        ff = open(folder + fl, \"r\")\n",
    "        data = data + parse(ff.read())\n",
    "        ff.close()\n",
    "    print(len(data))\n",
    "    feature_data_ud, labelSet = get_feature_data_ud(data)\n",
    "    print(labelSet)\n",
    "    X_train_ud, X_test_ud = train_test_split(feature_data_ud, test_size=0.25, random_state=42)\n",
    "    train_sentences_ud = get_sentences(X_train_ud)\n",
    "    test_sentences_ud = get_sentences(X_test_ud)\n",
    "    return train_sentences_ud, test_sentences_ud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sentences_dial, test_sentences_dial = get_dial_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60551\n",
      "ok2\n",
      "{'DET', 'CONJ', 'NOUN', 'ADV', 'VERB', 'ADJ', 'PROPN', 'PRON', 'ADP', 'INTJ', 'PART', 'SCONJ', 'NUM', 'SYM', 'X', 'PUNCT', 'AUX'}\n"
     ]
    }
   ],
   "source": [
    "train_sentences_ud, test_sentences_ud = get_ud_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok2\n"
     ]
    }
   ],
   "source": [
    "train_sentences_wiki, test_sentences_wiki = get_wiki_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_ud2Idx = {'VERB':0, 'PART':1, 'NUM':2, 'X':3, 'INTJ':4, 'NOUN':5, 'DET':6, 'PUNCT':7, 'CONJ':8, 'PROPN':9, 'ADJ':10, 'PRON':11, 'SCONJ':12, 'SYM':13, 'AUX':14, 'ADV':15, 'ADP':16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "chars = set()\n",
    "for dataset in [train_sentences_dial, test_sentences_dial]:\n",
    "    for sentence in dataset:\n",
    "        for token in sentence:\n",
    "            for c in token['lemma']:\n",
    "                chars.add(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(chars))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "charEmbeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "words = {}\n",
    "\n",
    "for dataset in [train_sentences_dial, test_sentences_dial]:\n",
    "    for sentence in dataset:\n",
    "        for token in sentence:\n",
    "            words[token['lemma'].lower()+'_'+pos_tags[token['pos']]] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_ud = {}\n",
    "\n",
    "for dataset in [train_sentences_ud, test_sentences_ud, train_sentences_dial, train_sentences_wiki, test_sentences_wiki]:\n",
    "    for sentence in dataset:\n",
    "        for token in sentence:\n",
    "            words_ud[token['forma'].lower()] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = {}\n",
    "\n",
    "for dataset in [train_sentences_ud, test_sentences_ud, train_sentences_dial, test_sentences_dial, train_sentences_wiki, test_sentences_wiki]:\n",
    "    for sentence in dataset:\n",
    "        for token in sentence:\n",
    "            words[token['forma'].lower()] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word2Idx = {}\n",
    "wordEmbeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262570"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad_words: 32250\n"
     ]
    }
   ],
   "source": [
    "vector = np.random.uniform(-0.25, 0.25, 300)\n",
    "wordEmbeddings.append(vector)\n",
    "bad_words = 0\n",
    "for word in words:\n",
    "    if word in w2v_model.vocab:# and word in words_ud:\n",
    "        wordEmbeddings.append(w2v_model[word])\n",
    "        word2Idx[word] = len(wordEmbeddings) - 1\n",
    "    else:\n",
    "        #print(word)\n",
    "        bad_words = bad_words + 1\n",
    "        word2Idx[word] = 0\n",
    "print(\"bad_words:\", bad_words)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "caseEmbeddings = []\n",
    "#caseFields = ['isupper', 'istitle', 'islower', 'isdigit', 'isalpha', 'isalnum', 'isLastWord', 'isFirstWord']\n",
    "caseFields = ['isupper', 'istitle', 'islower', 'isdigit', 'isalpha', 'isLastWord', 'isFirstWord']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def createMatrices_dial(sentences, word2Idx, label2Idx):\n",
    "    dataset = []\n",
    "    for sentence in sentences:\n",
    "        wordIndices = []    \n",
    "        caseIndices = []\n",
    "        labelIndices = []\n",
    "        for token in sentence:  \n",
    "            wordIdx = word2Idx[token['forma'].lower()]\n",
    "            wordIndices.append(wordIdx)\n",
    "            v = []\n",
    "            for field in caseFields:\n",
    "                v.append(token[field])\n",
    "            caseEmbeddings.append(v)\n",
    "            caseIndices.append(len(caseEmbeddings) - 1)\n",
    "            labelIndices.append([label2Idx[token['mark']], token['posStartInText'], token['FileName'], token['len']])\n",
    "        dataset.append([wordIndices, caseIndices, labelIndices]) \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createMatrices_wiki(sentences, word2Idx, label2Idx):\n",
    "    dataset = []\n",
    "    for sentence in sentences:\n",
    "        wordIndices = []    \n",
    "        caseIndices = []\n",
    "        labelIndices = []\n",
    "        for token in sentence:  \n",
    "            wordIdx = word2Idx[token['forma'].lower()]\n",
    "            wordIndices.append(wordIdx)\n",
    "            v = []\n",
    "            for field in caseFields:\n",
    "                v.append(token[field])\n",
    "            caseEmbeddings.append(v)\n",
    "            caseIndices.append(len(caseEmbeddings) - 1)\n",
    "            labelIndices.append([label2Idx[token['mark']]])\n",
    "        dataset.append([wordIndices, caseIndices, labelIndices]) \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createMatrices_ud(sentences, word2Idx, label2Idx):\n",
    "    dataset = []\n",
    "    for sentence in sentences:\n",
    "        wordIndices = []    \n",
    "        caseIndices = []\n",
    "        labelIndices = []\n",
    "        for token in sentence:  \n",
    "            wordIdx = word2Idx[token['forma'].lower()]\n",
    "            wordIndices.append(wordIdx)\n",
    "            v = []\n",
    "            for field in caseFields:\n",
    "                v.append(token[field])\n",
    "            caseEmbeddings.append(v)\n",
    "            caseIndices.append(len(caseEmbeddings) - 1)\n",
    "            labelIndices.append([label_ud2Idx[token['pos']]])\n",
    "        dataset.append([wordIndices, caseIndices, labelIndices]) \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def createMatrices1(sentences, word2Idx, label2Idx):\n",
    "    dataset = []\n",
    "    for sentence in sentences:\n",
    "        wordIndices = []    \n",
    "        caseIndices = []\n",
    "        labelIndices = []\n",
    "        charIndices = []\n",
    "        for token in sentence:  \n",
    "            wordIdx = word2Idx[token['lemma'].lower()+'_'+pos_tags[token['pos']]]\n",
    "            wordIndices.append(wordIdx)\n",
    "            v = []\n",
    "            for field in caseFields:\n",
    "                v.append(token[field])\n",
    "            w = np.zeros((len(chars)))\n",
    "            for c in token['lemma']:\n",
    "                #print(c)\n",
    "                w[char_indices[c]] = 1\n",
    "            charEmbeddings.append(w)\n",
    "            charIndices.append(len(charEmbeddings) - 1)\n",
    "            #v.append(link_words[token['link'].lower()])\n",
    "            #v.append(pos_words[token['pos'].lower()])\n",
    "            caseEmbeddings.append(v)\n",
    "            caseIndices.append(len(caseEmbeddings) - 1)\n",
    "            #labelIndices.append([label2Idx[token['mark']], token['posStartInText'], token['len']])\n",
    "            labelIndices.append([label2Idx[token['mark']], token['posStartInText'], token['FileName'], token['len'], label_bio2Idx[token['BIO']]])\n",
    "            #labelIndices.append(label2Idx[token['mark']])\n",
    "        dataset.append([wordIndices, caseIndices, labelIndices, charIndices]) \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "folder = '../data/'\n",
    "outputFilePath = folder + 'data_dial_wiki_all_embed.pkl.gz'\n",
    "embeddingsPklPath = folder + 'embeddings_dial_wiki_all_embed.pkl.gz'\n",
    "\n",
    "test_set_dial = createMatrices_dial(test_sentences_dial, word2Idx, label2Idx)\n",
    "train_set_dial = createMatrices_dial(train_sentences_dial, word2Idx, label2Idx)\n",
    "test_set_ud = createMatrices_ud(test_sentences_ud, word2Idx, label_ud2Idx)\n",
    "train_set_ud = createMatrices_ud(train_sentences_ud, word2Idx, label_ud2Idx)\n",
    "test_set_wiki = createMatrices_wiki(test_sentences_wiki, word2Idx, label2Idx)\n",
    "train_set_wiki = createMatrices_wiki(train_sentences_wiki, word2Idx, label2Idx)\n",
    "\n",
    "\n",
    "\n",
    "#embeddings = {'wordEmbeddings': wordEmbeddings, 'word2Idx': word2Idx,\n",
    "#              'caseEmbeddings': caseEmbeddings, 'label2Idx': label2Idx,\n",
    "#              'charEmbeddings': charEmbeddings, 'label_bio2Idx': label_bio2Idx}\n",
    "embeddings = {'wordEmbeddings': wordEmbeddings, 'word2Idx': word2Idx,\n",
    "              'caseEmbeddings': caseEmbeddings, 'label2Idx': label2Idx,\n",
    "              'label_ud2Idx': label_ud2Idx}\n",
    "\n",
    "f = gzip.open(embeddingsPklPath, 'wb')\n",
    "pkl.dump(embeddings, f, -1)\n",
    "f.close()\n",
    "\n",
    "f = gzip.open(outputFilePath, 'wb')\n",
    "pkl.dump(test_set_dial, f, -1)\n",
    "pkl.dump(train_set_dial, f, -1)\n",
    "pkl.dump(test_set_ud, f, -1)\n",
    "pkl.dump(train_set_ud, f, -1)\n",
    "pkl.dump(test_set_wiki, f, -1)\n",
    "pkl.dump(train_set_wiki, f, -1)\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
